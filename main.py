# -*- coding: utf-8 -*-
"""Project_02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZrX471KaoI4ns7DYFqZwsmw9qUwlTiuO
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd # data manipulation and analysis
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

"""1. **Data Input** - The dataset contains  information on various car
features, including mpg (miles per gallon), cylinders, displacement, horsepower, weight,acceleration, model year, and origin.

"""

#Importing the data, reporting the first few and last few data lines
data = pd.read_csv('https://archive.ics.uci.edu/static/public/9/data.csv')
data

data.shape # dimension of data

"""2. Data Cleaning & Preparation"""

data.info()

"""(a) Inspecting the dataset and checking for missing or incorrect values"""

import numpy as np
data.replace(("?","NA"), np.nan, inplace = True)

#Check for blank values
data.isnull().sum()

data.count()

#visualize missing values
import missingno as msno
msno.matrix(data)

"""Handling missing values"""

# Calculate the median of the 'horsepower' column
median_horsepower = data['horsepower'].median()

# Replace missing values in 'horsepower' with the median
data.loc[:, 'horsepower'] = data['horsepower'].fillna(median_horsepower)

# Verifying if there are any more missing values
missing_values_after = data.isnull().sum()

# This should return an empty result if no missing values remain
missing_values_after[missing_values_after > 0]

#save the cleaned data in csv
data.to_csv("/content/drive/MyDrive/Advanced ML/auto_mpg.csv")

data.info()

data.isnull().sum()

#now checking if there is any missing data (graphically)
msno.matrix(data)

"""  3. Feature Engineering

"""

# (a) Add power-to-weight ratio feature
data['power_to_weight'] = data['horsepower'] / data['weight']

# Add interaction between horsepower and cylinders
data['hp_cylinders_interaction'] = data['horsepower'] * data['cylinders']

# Display the first few rows to confirm the new features
data[['car_name', 'power_to_weight', 'hp_cylinders_interaction']].head()

# (b) Convert the 'origin' column into dummy variables
data_with_dummies = pd.get_dummies(data, columns=['origin'], drop_first=True)

# Display the first few rows to check the changes
data_with_dummies.head()

# Checking the distribution of the target variable
import matplotlib.pyplot as plt

# Plot the distribution of the target variable 'mpg'
plt.figure(figsize=(8, 6))
plt.hist(data_with_dummies['mpg'], bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of MPG (Miles per Gallon)')
plt.xlabel('MPG')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""The above histogram appears to be slightly skewed to the right, with most of the values clustered between 10 and 30 MPG."""

# (d) Use normalization or standardization to scale the features if necessary.
from sklearn.preprocessing import StandardScaler

# List of numerical features to scale (excluding the target 'mpg' and car_name)
features_to_scale = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year',
                     'power_to_weight', 'hp_cylinders_interaction', 'origin_2', 'origin_3']

# Initialize the standard scaler
scaler = StandardScaler()

# Apply scaling to the features
data_with_dummies[features_to_scale] = scaler.fit_transform(data_with_dummies[features_to_scale])

# Display the first few rows to confirm scaling
data_with_dummies[features_to_scale].head()

"""The features have been successfully standardize."""

#(e) I'll split the dataset into 75% training and 25% testing. I'll also ensure that the target variable mpg is separated from the features.
from sklearn.model_selection import train_test_split

# Define the target variable and features
X = data_with_dummies.drop(columns=['mpg', 'car_name'])
y = data_with_dummies['mpg']

# Split the dataset into 75% training and 25% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Display the shapes of the training and testing sets
(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""*   The Training set has 294 samples for the target variable(mpg) with 10 features (independent variables)
*  The Testing Set has 98 samples for the target variable(mpg) and 10 features (independent variables).

4. Linear Regression (Traditional)
"""

# Model 1
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# (a) Fit a traditional linear regression model using the training data
model1 = LinearRegression()
model1.fit(X_train_scaled, y_train)

# (b) Predict the mpg values for the test set
y_pred1 = model1.predict(X_test_scaled)

# Evaluate the model using Mean Squared Error (MSE) and R-squared (R2) score
mse1 = np.mean((y_pred1 - y_test) ** 2)
r2_1 = r2_score(y_test, y_pred1)

print("Traditional Linear Regression:")
print(f"Mean Squared Error: {mse1:.4f}")
print(f"R-squared: {r2_1:.4f}")
print("Linear Regression Parameters:", model1.coef_, model1.intercept_)

"""From the above we note that Mean Squared Error (MSE): 6.61 and the R-squared (R^2) score: 0.88. The R score indicates that about 88% of the variance in the target variable (mpg) is explained by the traditional linear regression model.

5. Gradient Descent and hyper-parameter tuning

    (a) Model2: Implementing the Regression Model with Gradient Descent Optimizer
"""

# Gradient Descent Implementation
class GradientDescentLinearRegression:
    def __init__(self, learning_rate=0.01, iterations=1000):
        self.learning_rate = learning_rate
        self.iterations = iterations
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        # Gradient Descent Loop
        for i in range(self.iterations):
            y_predicted = np.dot(X, self.weights) + self.bias
            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1 / n_samples) * np.sum(y_predicted - y)

            # Update weights and bias
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

    def predict(self, X):
        return np.dot(X, self.weights) + self.bias

# Initialize the model
model2 = GradientDescentLinearRegression(learning_rate=0.01, iterations=10000)
model2.fit(X_train_scaled, y_train)

# Make predictions
y_pred2 = model2.predict(X_test_scaled)

# Evaluate the model
mse_gd = mean_squared_error(y_test, y_pred2)
r2_gd = r2_score(y_test, y_pred2)

print(f"Gradient Descent MSE: {mse_gd:.4f}")
print(f"Gradient Descent R-squared: {r2_gd:.4f}")

"""  (b) Experiment with Different Hyperparameters (Learning Rates, Iterations, etc.)"""

# Experiment with different learning rates and iterations
learning_rates = [0.001, 0.01, 0.1]
iterations_list = [1000, 5000, 10000]

for lr in learning_rates:
    for iters in iterations_list:
        model2 = GradientDescentLinearRegression(learning_rate=lr, iterations=iters)
        model2.fit(X_train_scaled, y_train)
        y_pred2 = model2.predict(X_test_scaled)

        mse_gd = mean_squared_error(y_test, y_pred2)
        r2_gd = r2_score(y_test, y_pred2)

        print(f"Learning Rate: {lr}, Iterations: {iters} --> MSE: {mse_gd:.4f}, R-squared: {r2_gd:.4f}")

"""  (c) Predict MPG Values and Evaluate with MSE and R-Squared"""

# Predictions on the test set
y_pred2 = model2.predict(X_test_scaled)

# Calculate Mean Squared Error and R-squared Score
mse_gd = mean_squared_error(y_test, y_pred2)
r2_gd = r2_score(y_test, y_pred2)

print(f"Predicted MPG values: {y_pred2}")
print(f"Gradient Descent Mean Squared Error: {mse_gd:.4f}")
print(f"Gradient Descent R-squared Score: {r2_gd:.4f}")

"""  Visualize the results"""

# Plot predictions vs actual values
plt.scatter(y_test, y_pred2, color='blue', label='Predictions vs Actual')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', label='Ideal Fit Line', linewidth=2)
plt.title('Gradient Descent: Predictions vs Actual MPG')
plt.xlabel('Actual MPG')
plt.ylabel('Predicted MPG')
plt.legend()
plt.show()

"""6. Comparison

  (a) Compare the MSE and R-squared scores of Model1 and Model2
  
  (b) Which Model Performs Better?

(c) Plot the Predicted MPG Values versus the Actual MPG Values for Both Models
"""

# Plot predicted vs actual for both models
plt.figure(figsize=(10,5))

# Model 1 Predictions
plt.subplot(1, 2, 1)
plt.scatter(y_test, y_pred1, color='blue', label='Model 1 Predictions')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', label='Ideal Fit', linewidth=2)
plt.title('Model 1: Predictions vs Actual MPG')
plt.xlabel('Actual MPG')
plt.ylabel('Predicted MPG')
plt.legend()

# Model 2 Predictions
plt.figure(figsize=(10,5))
plt.subplot(1, 2, 2)
plt.scatter(y_test, y_pred2, color='green', label='Model 2 Predictions')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', label='Ideal Fit', linewidth=2)
plt.title('Model 2: Predictions vs Actual MPG')
plt.xlabel('Actual MPG')
plt.ylabel('Predicted MPG')
plt.legend()

plt.tight_layout()
plt.show()

"""  (d) Create Scatter Plots to Visualize the Prediction Errors"""

# Calculate errors for both models
errors_model1 = y_test - y_pred1
errors_model2 = y_test - y_pred2

# Scatter plot for prediction errors
plt.figure(figsize=(10,5))

# Model 1 Errors
plt.subplot(1, 2, 1)
plt.scatter(y_test, errors_model1, color='blue', label='Model 1 Errors')
plt.axhline(y=0, color='red', linestyle='--')
plt.title('Model 1: Prediction Errors')
plt.xlabel('Actual MPG')
plt.ylabel('Prediction Error')
plt.legend()

# Model 2 Errors
plt.subplot(1, 2, 2)
plt.scatter(y_test, errors_model2, color='green', label='Model 2 Errors')
plt.axhline(y=0, color='red', linestyle='--')
plt.title('Model 2: Prediction Errors')
plt.xlabel('Actual MPG')
plt.ylabel('Prediction Error')
plt.legend()

plt.tight_layout()
plt.show()